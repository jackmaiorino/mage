$env:MTG_AI_LOG_LEVEL="INFO"
$env:METRICS_PORT=9090

# Model profile - all artifacts (models + logs) go under rl/profiles/Pauper-Elves/
$env:MODEL_PROFILE="Pauper-Elves"

# Inference workers (each = 1 Python proc + 1 model replica on GPU)
$env:PY4J_BASE_PORT=25334
$env:INFER_WORKERS=1
$env:INFER_STARTUP_THREADS=1
$env:PY_SCORE_TIMEOUT_MS=0   # No timeout - games wait as long as needed
$env:MODEL_RELOAD_EVERY_MS=2000

# Snapshots (keep VRAM stable)
$env:SNAPSHOT_CACHE_SIZE=0

# Learner queue (don't stall runners)
$env:TRAIN_QUEUE_MAX_EPISODES=160
$env:TRAIN_QUEUE_DROP_ON_FULL=1
$env:TRAIN_QUEUE_OFFER_TIMEOUT_MS=0

# Learner micro-batching
$env:LEARNER_BATCH_MAX_EPISODES=16
$env:LEARNER_BATCH_MAX_STEPS=512

$env:MODEL_SYNC_EVERY_TRAIN_STEPS=100
$env:MODEL_SYNC_EVERY_MS=5000

# Java-side inference batching (performance)
$env:PY_BATCH_MAX_SIZE=256
$env:PY_BATCH_TIMEOUT_MS=10
$env:PY_BATCH_LOG_LEVEL="WARNING"

# Python-side auto-batching (avoid paging/shared RAM)
$env:AUTO_BATCH_ENABLE=1
$env:AUTO_AVOID_PAGING=1
$env:AUTO_TARGET_USED_FRAC=0.75
$env:AUTO_MIN_FREE_MB=512

# Model architecture (v2 small defaults: 2-layer, d_model=128, ~2M params)
# $env:MODEL_D_MODEL=128
# $env:MODEL_NHEAD=4
# $env:MODEL_NUM_LAYERS=2
# $env:MODEL_DIM_FEEDFORWARD=512

# Entropy schedule
$env:ENTROPY_START=0.10
$env:ENTROPY_END=0.01
$env:ENTROPY_DECAY_STEPS=60000

# Learning rate
$env:ACTOR_LR="3e-4"
$env:OTHER_LR="3e-4"

# Training settings
$env:USE_GAE=0
$env:GAE_AUTO_ENABLE=1
$env:GAE_AUTO_THRESHOLD=0.65
$env:GAE_AUTO_MIN_SAMPLES=200

$env:LOSS_SCHEDULE_ENABLE=0
$env:CRITIC_WARMUP_STEPS=0
$env:FREEZE_ENCODER_IN_WARMUP=0

# Loss coefficients
$env:VALUE_LOSS_COEF=1.0
$env:POLICY_LOSS_COEF=1.0
$env:ENTROPY_LOSS_MULT=1.0
$env:ADV_CLIP_MAX=5.0
$env:MAX_GRAD_NORM=1.0

# Debug
$env:VALUE_POSTUPDATE_DIAG_EVERY=200
$env:VALUE_TARGET_DIAG_EVERY=200
$env:VALUE_GRAD_DIAG_EVERY=200
$env:SCORE_DIAG_EVERY=2000

$env:PY_BACKEND_MODE="multi"
$env:MULLIGAN_DEVICE="cpu"

# League / sampler
$env:GAME_TIMEOUT_SEC=600
$env:LEAGUE_BASELINE_GAMES_PER_MATCHUP=6
$env:LEAGUE_HEURISTIC_SKILLS_PRE="1"

# Single-deck training: RL agent plays Elves only, opponents use full pool
$env:RL_AGENT_DECK_LIST="Mage.Server.Plugins/Mage.Player.AIRL/src/mage/player/ai/decks/PauperSubset/decklist.elves.txt"

# Ladder mode: skill-only progression
$env:OPPONENT_SAMPLER="ladder"
$env:LADDER_SKILLS="1,2,3,4,5,6,7"
$env:LADDER_PROMOTE_WR=0.55
$env:LADDER_TICK_EPISODES=5000
$env:LADDER_GAMES_PER_MATCHUP=6
$env:LADDER_MIX_LOWER_P=0.20

# Run
.\scripts\rl-train.ps1 -Profile perf -LogLevel INFO -ModelProfile "Pauper-Elves" `
  -TotalEpisodes 1000000 -NumGameRunners 48 `
  -DeckListFile "Mage.Server.Plugins/Mage.Player.AIRL/src/mage/player/ai/decks/PauperSubset/decklist.txt"
