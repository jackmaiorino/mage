  # ============================================================
  # Vintage Cube Draft Training Commands
  # Usage: paste env vars into terminal, then run the mvn line
  # ============================================================

  $env:MTG_AI_LOG_LEVEL="INFO"

  # -------------------------------------------------------
  # Model Profiles
  # -------------------------------------------------------
  # Game model profile (used to pilot drafted decks in self-play)
  $env:MODEL_PROFILE="Vintage-Cube"

  # Draft model profile (picks + deck construction)
  # Artifacts go to: rl/profiles/Vintage-Cube/models/ (draft subdir)
  $env:DRAFT_MODEL_PROFILE="Vintage-Cube"

  # Cube deck list (loaded via CubeFromDeck)
  $env:CUBE_DECK_PATH="Mage.Server.Plugins/Mage.Player.AIRL/src/mage/player/ai/decks/Vintage/Cube/LSVCube.txt"

  # -------------------------------------------------------
  # Draft Training Loop (DraftTrainer)
  # -------------------------------------------------------
  # Total draft episodes (each = 1 full 8-player draft + N eval games)
  $env:DRAFT_TOTAL_EPISODES=10000

  # Evaluation games played per draft episode (RL-drafted deck vs heuristic deck)
  $env:DRAFT_GAMES_PER_EPISODE=3

  # Save draft model every N episodes
  $env:DRAFT_SAVE_EVERY=50

  # Log full draft decision logs every N episodes (pack contents, scores, pick order)
  $env:DRAFT_LOG_FREQUENCY=10
  # Optional: detailed per-episode reward-game logs from DraftTrainer
  # (written to logs/games/draft_eval)
  # Set to 1 for every evaluation game.
  $env:DRAFT_EVAL_GAME_LOGGING=0
  # Normal-training style logging cadence: log game #1 and every Xth game after.
  # Falls back to GAME_LOG_FREQUENCY if not set.
  $env:DRAFT_GAME_LOG_FREQUENCY=200

  # -------------------------------------------------------
  # Periodic benchmark tick (separate from per-episode reward games)
  # -------------------------------------------------------
  # Run a benchmark every N draft episodes (like ladder-style periodic checks).
  $env:DRAFT_BENCHMARK_ENABLE=1
  $env:DRAFT_BENCHMARK_EVERY=5000
  # Each benchmark runs N fresh drafts, with M games per draft.
  $env:DRAFT_BENCHMARK_DRAFTS=12
  $env:DRAFT_BENCHMARK_GAMES_PER_DRAFT=3
  # If benchmark WR >= threshold, benchmark tier increments.
  $env:DRAFT_BENCHMARK_PROMOTE_WR=0.55
  # Optional benchmark game logs (written to logs/games/benchmark).
  $env:DRAFT_BENCHMARK_GAME_LOGGING=0
  # If >0, logs benchmark game #1 and every Xth benchmark game.
  $env:DRAFT_BENCHMARK_GAME_LOG_FREQUENCY=0

  # -------------------------------------------------------
  # Co-train game + mulligan models during cube DraftTrainer runs
  # -------------------------------------------------------
  # Immediate on-policy co-training from draft reward games.
  $env:DRAFT_COTRAIN_GAME_MODEL=1
  $env:DRAFT_COTRAIN_MULLIGAN_MODEL=1
  # Train both sides in self-play reward games.
  $env:DRAFT_COTRAIN_BOTH_SIDES=1
  # Keep benchmark runs as no-train by default.
  $env:DRAFT_COTRAIN_ON_BENCHMARK=0
  # Mulligan checkpoint cadence for co-training updates.
  $env:DRAFT_MULLIGAN_SAVE_INTERVAL=100

  # Save play + mulligan models periodically during DraftTrainer,
  # so Ctrl+C doesn't rely solely on shutdown hook race ordering.
  $env:DRAFT_SAVE_GAME_MODELS=1
  $env:DRAFT_SAVE_GAME_MODELS_EVERY=50

  # Shutdown-hook checkpoint controls (best-effort on Ctrl+C).
  $env:DRAFT_SHUTDOWN_SAVE_HOOK=1
  $env:DRAFT_SHUTDOWN_SAVE_GAME_MODELS=1

  # Write draft_stats.csv every N episodes
  $env:DRAFT_STATS_EVERY=10

  # Rolling window for winrate tracking
  $env:DRAFT_WINRATE_WINDOW=100

  # -------------------------------------------------------
  # Construction Curriculum (land floor)
  # -------------------------------------------------------
  # Enforce a minimum land count early, then decay to free choice.
  $env:DRAFT_MIN_LANDS_ENABLE=1
  $env:DRAFT_MIN_LANDS_START=12
  $env:DRAFT_MIN_LANDS_END=0
  $env:DRAFT_MIN_LANDS_DECAY_EPISODES=5000

  # -------------------------------------------------------
  # Draft Model Py4J Bridge
  # -------------------------------------------------------
  # Port for Java <-> Python draft model communication (separate from game model port)
  # Game model uses: BASE_PORT (learner) + BASE_PORT+1..BASE_PORT+INFER_WORKERS (inference)
  # e.g. with BASE=25334, INFER_WORKERS=1: game uses 25334 + 25335
  # Draft model must be above that range. Default: 25360 (clear of up to 16 inference workers)
  $env:DRAFT_PY4J_PORT=25360

  # -------------------------------------------------------
  # Draft Model Hyperparameters
  # (defaults in draft_py4j_entry_point.py, override here if needed)
  # -------------------------------------------------------
  # Architecture: 256d, 4-head, 4-layer (~500K-1M params, smaller than game model)
  # $env:DRAFT_D_MODEL=256
  # $env:DRAFT_NHEAD=4
  # $env:DRAFT_NUM_LAYERS=4
  # $env:DRAFT_CAND_DIM=32

  # -------------------------------------------------------
  # Draft Model Training Hyperparameters
  # -------------------------------------------------------
  $env:DRAFT_LR="3e-4"
  $env:DRAFT_PPO_EPSILON="0.2"
  $env:DRAFT_VALUE_LOSS_COEF="5.0"
  $env:DRAFT_ENTROPY_COEF="0.05"   # Higher entropy early; draft exploration is important
  $env:DRAFT_MAX_GRAD_NORM="1.0"
  $env:DRAFT_GAMMA="0.99"
  $env:DRAFT_GAE_LAMBDA="0.95"

  # -------------------------------------------------------
  # Game Model Py4J (for self-play game evaluation)
  # Same settings as regular training but for the co-training context
  # -------------------------------------------------------
  $env:PY4J_BASE_PORT=25334
  $env:INFER_WORKERS=1
  $env:INFER_STARTUP_THREADS=1
  $env:PY_SCORE_TIMEOUT_MS=0
  $env:MODEL_RELOAD_EVERY_MS=2000

  $env:SNAPSHOT_CACHE_SIZE=0

  # Game model entropy (for self-play games evaluating draft outcomes)
  $env:ENTROPY_START=0.10
  $env:ENTROPY_END=0.01
  $env:ENTROPY_DECAY_STEPS=60000

  # Game model learning rates
  $env:ACTOR_LR="3e-4"
  $env:OTHER_LR="3e-4"

  # -------------------------------------------------------
  # Java-side inference batching
  # -------------------------------------------------------
  $env:PY_BATCH_MAX_SIZE=256
  $env:PY_BATCH_TIMEOUT_MS=10
  $env:PY_BATCH_LOG_LEVEL="WARNING"

  # -------------------------------------------------------
  # Python auto-batching (memory safety)
  # -------------------------------------------------------
  $env:AUTO_BATCH_ENABLE=1
  $env:AUTO_AVOID_PAGING=1
  $env:AUTO_TARGET_USED_FRAC=0.75
  $env:AUTO_MIN_FREE_MB=512

  # -------------------------------------------------------
  # Game model loss / GAE settings
  # -------------------------------------------------------
  $env:USE_GAE=0
  $env:GAE_AUTO_ENABLE=1
  $env:GAE_AUTO_THRESHOLD=0.65
  $env:GAE_AUTO_MIN_SAMPLES=200

  $env:LOSS_SCHEDULE_ENABLE=0
  $env:CRITIC_WARMUP_STEPS=0
  $env:FREEZE_ENCODER_IN_WARMUP=0

  $env:VALUE_LOSS_COEF=1.0
  $env:POLICY_LOSS_COEF=1.0
  $env:ENTROPY_LOSS_MULT=1.0
  $env:ADV_CLIP_MAX=5.0
  $env:MAX_GRAD_NORM=1.0

  # -------------------------------------------------------
  # Multi-backend: inference + learner run in parallel
  # -------------------------------------------------------
  $env:PY_BACKEND_MODE="multi"
  $env:MULLIGAN_DEVICE="cpu"

  # -------------------------------------------------------
  # Debug diagnostics
  # -------------------------------------------------------
  $env:VALUE_POSTUPDATE_DIAG_EVERY=200
  $env:VALUE_TARGET_DIAG_EVERY=200
  $env:VALUE_GRAD_DIAG_EVERY=200
  $env:SCORE_DIAG_EVERY=2000

  # Game timeout for self-play evaluation games (Vintage cards can be slower)
  $env:GAME_TIMEOUT_SEC=900

  # -------------------------------------------------------
  # Run DraftTrainer
  # Note: NumGameRunners controls parallel draft workers
  # Fewer runners than regular training (each draft episode is heavier)
  # -------------------------------------------------------
  .\scripts\rl-train.ps1 -Profile perf -LogLevel INFO -ModelProfile "Vintage-Cube" `
    -TotalEpisodes 10000 -NumGameRunners 8 `
    -MainClass "mage.player.ai.rl.DraftTrainer"
